{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapetube\n",
    "\n",
    "videos = scrapetube.get_search(\"python\", limit=3)\n",
    "\n",
    "# for video in videos:\n",
    "#     print(video[\"videoId\"])\n",
    "videos = list(videos)\n",
    "item = videos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(videos)\n",
    "type(videos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avideos = [str(i) for i in videos]\n",
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_meta = {}\n",
    "video_meta[\"video_id\"] = item[\"videoId\"].strip()\n",
    "\n",
    "title = item[\"title\"][\"runs\"][0][\"text\"].strip()\n",
    "channel = item[\"longBylineText\"][\"runs\"][0][\"text\"].strip()\n",
    "at_channel = item[\"longBylineText\"][\"runs\"][0][\"navigationEndpoint\"][\"browseEndpoint\"][\n",
    "    \"canonicalBaseUrl\"\n",
    "].strip()\n",
    "view_count = item[\"viewCountText\"][\"simpleText\"]\n",
    "\n",
    "# print(title, channel)\n",
    "video_meta[\"title\"] = title\n",
    "video_meta[\"channel\"] = channel\n",
    "video_meta[\"at_channel\"] = at_channel\n",
    "video_meta[\"view_count\"] = view_count\n",
    "video_meta[\"video_url\"] = \"https://www.youtube.com/watch?v=\" + video_meta[\"video_id\"]\n",
    "video_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by_map = {\n",
    "        \"relevance\": \"A\",\n",
    "        \"upload_date\": \"I\",\n",
    "        \"view_count\": \"M\",\n",
    "        \"rating\": \"E\",\n",
    "    }\n",
    "\n",
    "results_type_map = {\n",
    "    \"video\": [\"B\", \"videoRenderer\"],\n",
    "    \"channel\": [\"C\", \"channelRenderer\"],\n",
    "    \"playlist\": [\"D\", \"playlistRenderer\"],\n",
    "    \"movie\": [\"E\", \"videoRenderer\"],\n",
    "}\n",
    "\n",
    "query = \"ประชุมรัฐาภา\"\n",
    "sort_by = \"view_count\"\n",
    "results_type = \"video\"\n",
    "param_string = f\"CA{sort_by_map[sort_by]}SAhA{results_type_map[results_type][0]}\"\n",
    "url = f\"https://www.youtube.com/results?search_query={query}&sp={param_string}\"\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- เที่ยวทะเล subtitle + cc\n",
    "- subtitle, cc -> sp=EgQoATAB\n",
    "- subtitle, cc, relevance -> sp=CAA SBhAB KAEwAQ%253D%253D\n",
    "- subtitle, cc, upload date ->  sp= CAI SBhAB KAEwAQ%253D%253D\n",
    "- subtitle, cc, view count -> sp=CAM SBhAB KAEwAQ%253D%253D\n",
    "- subtitle, cc, rating -> sp=CAE SBhAB KAEwAQ%253D%253D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- เที่ยวทะเล no filter\n",
    "- relevance -> sp=CAASAhAB\n",
    "- upload date -> sp=CAISAhAB\n",
    "- view count -> sp=CAMSAhAB\n",
    "- rating -> sp=CAESAhAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- คณิตศาสตร์ cc\n",
    "- cc -> sp=EgIwAQ%253D%253D\n",
    "- cc, relevance -> sp=CAA SBBAB MAE%253D\n",
    "- cc, upload date -> sp=CAI SBBAB MAE%253D\n",
    "- cc, view count -> sp=CAM SBBAB MAE%253D\n",
    "- cc, rating -> sp=CAE SBBAB MAE%253D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_params = {\n",
    "    \"relevance\" : \"CAASBhABKAEwAQ%253D%253D\",\n",
    "    \"upload_date\" : \"CAISBhABKAEwAQ%253D%253D\",\n",
    "    \"view_count\" : \"CAMSBhABKAEwAQ%253D%253D\",\n",
    "    \"rating\" : \"CAESBhABKAEwAQ%253D%253D\"\n",
    "}\n",
    "\n",
    "query = \"ประชุมสภา\"\n",
    "sort_by = \"upload_date\"\n",
    "results_type = \"video\"\n",
    "# param_string = f\"CA{sort_by_map[sort_by]}SBhA{results_type_map[results_type][0]}KAEwAQ%253D%253D\"\n",
    "# url = f\"https://www.youtube.com/results?search_query={query}&sp=CAMSBhABKAEwAQ%253D%253D\"\n",
    "url = f\"https://www.youtube.com/results?search_query={query}&sp=EgQoATAB\"\n",
    "# url = f\"https://www.youtube.com/results?search_query={query}&sp={sp_params[sort_by]}\"\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_params = {\n",
    "    \"creative_commons\" : \"EgIwAQ%253D%253D\",\n",
    "    \"relevance\" : \"CAASBBABMAE%253D\",\n",
    "    \"upload_date\" : \"CAISBBABMAE%253D\",\n",
    "    \"view_count\" : \"CAMSBBABMAE%253D\",\n",
    "    \"rating\" : \"CAESBBABMAE%253D\"\n",
    "}\n",
    "\n",
    "query = \"คณิตศาสตร์\"\n",
    "sort_by = \"creative_commons\"\n",
    "results_type = \"video\"\n",
    "# param_string = f\"CA{sort_by_map[sort_by]}SBhA{results_type_map[results_type][0]}KAEwAQ%253D%253D\"\n",
    "# url = f\"https://www.youtube.com/results?search_query={query}&sp=CAMSBhABKAEwAQ%253D%253D\"\n",
    "# url = f\"https://www.youtube.com/results?search_query={query}&sp=EgQoATAB\"\n",
    "# url = f\"https://www.youtube.com/results?search_query={query}&sp={sp_params[sort_by]}\"\n",
    "url = f\"https://www.youtube.com/results?search_query={query}&sp={sp_params[sort_by]}\"\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.randint(3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scrapetube import get_search_subtitle_cc\n",
    "\n",
    "query = \"ประชุมสภา\" # **\n",
    "limit = 3 # **\n",
    "sort_by = \"view_count\"\n",
    "proxy = {\n",
    "    \"http\" : \"http://103.25.210.233:9191\"\n",
    "}\n",
    "\n",
    "videos = get_search_subtitle_cc(\n",
    "    query=query,\n",
    "    limit=limit,\n",
    "    sleep=random.randint(5, 15),\n",
    "    sort_by=sort_by,\n",
    "    proxies=proxy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = []\n",
    "for video, url in videos:\n",
    "    print(url)\n",
    "    video_id.append(video[\"videoId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api import NoTranscriptFound\n",
    "\n",
    "subtitles = []\n",
    "for id in video_id:\n",
    "    transcript_list = YouTubeTranscriptApi.list_transcripts(id)\n",
    "    try:\n",
    "        print(\"finding manually created transcript\")\n",
    "        transcript = transcript_list.find_manually_created_transcript([\"th\"])\n",
    "    except NoTranscriptFound:\n",
    "        # Fallback to a generated transcript if manual transcript is not found\n",
    "        print(\"finding auto generated transcript\")\n",
    "        transcript = transcript_list.find_generated_transcript([\"th\"])\n",
    "\n",
    "    # Fetch and append the transcript content\n",
    "    subtitles.append(transcript.fetch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subtitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = YouTubeTranscriptApi.list_transcripts(\"MpDZakREtaA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.find_manually_created_transcript(language_codes=[\"th\"]).fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "# import joblib\n",
    "import json\n",
    "\n",
    "from time import sleep\n",
    "from scrapetube import get_search_cc\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import logging\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get today's date\n",
    "today_date = datetime.now()\n",
    "\n",
    "# Format it as a string (e.g., \"YYYY-MM-DD\")\n",
    "today_string = today_date.strftime(\"%Y-%m-%d\").replace(\"-\", \"\")\n",
    "\n",
    "print(today_string, type(today_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # try one query\n",
    "# query = query_strings[0]\n",
    "# limit = 50\n",
    "# sleep = random.randint(7, 20)\n",
    "# sp_filter = \"relevance\"\n",
    "# proxies = {\n",
    "#     # \"https\": \"https://72.10.160.174:13093\",\n",
    "#     # \"socks4\" : \"sock4://103.165.64.86:4153\",\n",
    "#     \"http\": \"http://103.25.210.233:9191\"\n",
    "# }\n",
    "# video_base_url = \"https://www.youtube.com/watch?v=\"\n",
    "\n",
    "\n",
    "# video_meta = get_search_cc(\n",
    "#     query=query, limit=limit, sleep=sleep, sp_filter=sp_filter, proxies=proxies\n",
    "# )\n",
    "# items = []\n",
    "# for v_meta in tqdm(video_meta, total=limit):\n",
    "#     item_dict = {}\n",
    "#     item_dict[\"video_id\"] = v_meta[\"videoId\"].strip()\n",
    "\n",
    "#     title = v_meta[\"title\"][\"runs\"][0][\"text\"].strip()\n",
    "#     channel = v_meta[\"longBylineText\"][\"runs\"][0][\"text\"].strip()\n",
    "#     at_channel = v_meta[\"longBylineText\"][\"runs\"][0][\"navigationEndpoint\"][\n",
    "#         \"browseEndpoint\"\n",
    "#     ][\"canonicalBaseUrl\"].strip()\n",
    "#     view_count = v_meta[\"viewCountText\"][\"simpleText\"]\n",
    "\n",
    "#     # print(title, channel)\n",
    "#     item_dict[\"title\"] = title\n",
    "#     item_dict[\"channel\"] = channel\n",
    "#     item_dict[\"at_channel\"] = at_channel\n",
    "#     item_dict[\"view_count\"] = view_count\n",
    "#     item_dict[\"video_url\"] = video_base_url + item_dict[\"video_id\"]\n",
    "#     items.append(item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Set up logging\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "# )\n",
    "\n",
    "\n",
    "# def get_cc_video_meta(\n",
    "#     queries: List[str],\n",
    "#     limit: int,\n",
    "#     sleep_min: int,\n",
    "#     sleep_max: int,\n",
    "#     sp_filter: str,\n",
    "#     results_type: str = \"video\",\n",
    "#     proxies: Optional[Dict[str, str]] = None,\n",
    "# ) -> Tuple[pd.DataFrame, List[dict]]:\n",
    "#     \"\"\"\n",
    "#     Retrieve video metadata for a list of queries using Scrapetube.\n",
    "\n",
    "#     Args:\n",
    "#         queries (List[str]): List of search queries to fetch metadata for.\n",
    "#         limit (int): Maximum number of results per query.\n",
    "#         sleep_min (int): Minimum sleep time between requests.\n",
    "#         sleep_max (int): Maximum sleep time between requests.\n",
    "#         sp_filter (str): Filter parameter for YouTube search.\n",
    "#         results_type (str, optional): Type of result to fetch (default is 'video').\n",
    "#         proxies (Optional[Dict[str, str]], optional): Proxy configuration (default is None).\n",
    "\n",
    "#     Returns:\n",
    "#         Tuple[List[pd.DataFrame], List[dict]]: A tuple containing:\n",
    "#             - A list of Pandas DataFrames with metadata.\n",
    "#             - A list of raw JSON responses.\n",
    "#     \"\"\"\n",
    "#     video_base_url = \"https://www.youtube.com/watch?v=\"\n",
    "#     all_dataframes = []\n",
    "#     all_jsons = []\n",
    "\n",
    "#     for query in queries:\n",
    "#         logging.info(f\"Searching YouTube for: {query}\")\n",
    "\n",
    "#         # Random sleep time for throttling\n",
    "#         sleep_time = random.randint(sleep_min, sleep_max)\n",
    "#         try:\n",
    "#             video_meta_generator = get_search_cc(\n",
    "#                 query=query,\n",
    "#                 limit=limit,\n",
    "#                 sleep=sleep_time,\n",
    "#                 sp_filter=sp_filter,\n",
    "#                 results_type=results_type,\n",
    "#                 proxies=proxies,\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Failed to fetch metadata for query '{query}': {e}\")\n",
    "#             continue\n",
    "\n",
    "#         items = []\n",
    "#         logging.info(\"Fetching metadata...\")\n",
    "#         for video_meta in tqdm(\n",
    "#             video_meta_generator, total=limit, desc=f\"Processing {query}\"\n",
    "#         ):\n",
    "#             try:\n",
    "#                 # Extract metadata\n",
    "#                 video_id = video_meta[\"videoId\"].strip()\n",
    "#                 title = video_meta[\"title\"][\"runs\"][0][\"text\"].strip()\n",
    "#                 channel = video_meta[\"longBylineText\"][\"runs\"][0][\"text\"].strip()\n",
    "#                 at_channel = video_meta[\"longBylineText\"][\"runs\"][0][\n",
    "#                     \"navigationEndpoint\"\n",
    "#                 ][\"browseEndpoint\"][\"canonicalBaseUrl\"].strip()\n",
    "#                 view_count = video_meta[\"viewCountText\"][\"simpleText\"]\n",
    "\n",
    "#                 # Add to item list\n",
    "#                 items.append(\n",
    "#                     {\n",
    "#                         \"query_string\" : query,\n",
    "#                         \"video_id\": video_id,\n",
    "#                         \"title\": title,\n",
    "#                         \"channel\": channel,\n",
    "#                         \"at_channel\": at_channel,\n",
    "#                         \"view_count\": view_count,\n",
    "#                         \"video_url\": f\"{video_base_url}{video_id}\",\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#                 # Append raw JSON for detailed analysis later\n",
    "#                 all_jsons.append(video_meta)\n",
    "\n",
    "#             except KeyError as e:\n",
    "#                 logging.warning(f\"KeyError encountered while processing a video: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#         # Convert to DataFrame and append\n",
    "#         if items:\n",
    "#             all_dataframes.append(pd.DataFrame(items))\n",
    "\n",
    "#         # Sleep to avoid being blocked\n",
    "#         sleep_interval = random.randint(3, 7)\n",
    "#         logging.info(f\"Sleeping for {sleep_interval} seconds...\")\n",
    "#         sleep(sleep_interval)\n",
    "\n",
    "#     logging.info(\"Finished fetching metadata.\")\n",
    "#     return pd.concat(all_dataframes), all_jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "    # Random sleep time for throttling\n",
    "    # sleep_time = random.randint(sleep_min, sleep_max)\n",
    "    # proxy = random.choice(proxies)\n",
    "    # try:\n",
    "    #     video_meta_generator = get_search_cc(\n",
    "    #         query=query,\n",
    "    #         limit=limit,\n",
    "    #         sleep=sleep_time,\n",
    "    #         sp_filter=sp_filter,\n",
    "    #         results_type=results_type,\n",
    "    #         proxies=proxy,\n",
    "    #     )\n",
    "\n",
    "    #     video_meta_list = list(video_meta_generator)\n",
    "    # except Exception as e:\n",
    "    #     logging.warning(\n",
    "    #         f\"Error during get search cc for '{query}'. Retrying new proxy...\"\n",
    "    #     )\n",
    "    #     proxy = random.choice([p for p in proxies if p != proxy])\n",
    "    #     video_meta_generator = get_search_cc(\n",
    "    #         query=query,\n",
    "    #         limit=limit,\n",
    "    #         sleep=sleep_time,\n",
    "    #         sp_filter=sp_filter,\n",
    "    #         results_type=results_type,\n",
    "    #         proxies=proxy,\n",
    "    #     )\n",
    "    #     video_meta_list = list(video_meta_generator)\n",
    "    # Keep track of proxies tried\n",
    "    # if not video_meta_list:\n",
    "    #     logging.warning(f\"No results found for query '{query}'. Skipping...\")\n",
    "    #     return pd.DataFrame(), []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proxies_1 = [\n",
    "    {\"http\": \"http://171.7.103.51:8080\"},\n",
    "    {\"http\": \"http://49.48.64.25:8080\"},\n",
    "    {\"http\": \"http://182.52.108.88:8180\"},\n",
    "    {\"http\": \"http://103.25.210.233:9191\"},\n",
    "    {\"http\": \"http://67.43.227.226:30373\"},\n",
    "    {\"http\": \"http://116.202.121.34:3128\"},\n",
    "    {\"http\": \"http://116.108.3.96:10017\"},\n",
    "    {\"http\": \"http://31.47.58.37:80\"},\n",
    "    {\"http\": \"http://213.218.255.99:80\"},\n",
    "    {\"http\": \"http://72.10.160.94:8355\"},\n",
    "    {\"http\": \"http://47.74.152.29:8888\"},\n",
    "    {\"http\": \"http://190.103.177.131:80\"},\n",
    "    {\"http\": \"http://123.30.154.171:7777\"},\n",
    "    {\"http\": \"http://50.174.7.156:80\"},\n",
    "    {\"http\": \"http://50.231.104.58:80\"},\n",
    "    {\"http\": \"http://68.185.57.66:80\"},\n",
    "]\n",
    "\n",
    "path = \"proxies.txt\"\n",
    "with open(path, \"r\") as f:\n",
    "    p_list = f.read().splitlines()\n",
    "\n",
    "proxies_2 = [ {\"http\" : \"http://\" + p} for p in p_list]\n",
    "proxies = proxies_1 + proxies_2\n",
    "proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def fetch_video_meta(\n",
    "#     query: str,\n",
    "#     limit: int,\n",
    "#     sleep_min: int,\n",
    "#     sleep_max: int,\n",
    "#     sp_filter: str,\n",
    "#     results_type: str,\n",
    "#     proxies: list[dict[str, str]],\n",
    "#     video_base_url: str,\n",
    "#     file_path_csv: str,\n",
    "#     file_path_json: str,\n",
    "# ) -> tuple[pd.DataFrame, list[dict]]:\n",
    "#     \"\"\"\n",
    "#     Fetch video metadata for a single query.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         logging.info(f\"Searching YouTube for: {query}\")\n",
    "#         sleep_time = random.randint(sleep_min, sleep_max)\n",
    "\n",
    "#         # Keep track of proxies tried\n",
    "#         proxies_tried = set()\n",
    "#         video_meta_list = []\n",
    "#         while len(proxies_tried) < len(proxies):\n",
    "#             # Select a random proxy that has not been tried\n",
    "#             proxy = random.choice(\n",
    "#                 [p for p in proxies if tuple(p.items()) not in proxies_tried]\n",
    "#             )\n",
    "#             proxies_tried.add(tuple(proxy.items()))\n",
    "\n",
    "#             try:\n",
    "#                 video_meta_list = list(\n",
    "#                     get_search_cc(\n",
    "#                         query=query,\n",
    "#                         limit=limit,\n",
    "#                         sleep=sleep_time,\n",
    "#                         sp_filter=sp_filter,\n",
    "#                         results_type=results_type,\n",
    "#                         proxies=proxy,\n",
    "#                     )\n",
    "#                 )\n",
    "#                 # If the list is not empty, break out of the loop\n",
    "#                 if video_meta_list:\n",
    "#                     logging.info(f\"Found result for {query}\")\n",
    "#                     break\n",
    "#                 else:\n",
    "#                     logging.warning(\n",
    "#                         f\"Proxy {proxy} returned an empty result for query '{query}'. There is no cc videos\"\n",
    "#                     )\n",
    "#                     break\n",
    "#             except Exception as e:\n",
    "#                 logging.warning(\n",
    "#                     f\"Proxy {proxy} failed for query '{query}': {e}. Trying another proxy...\"\n",
    "#                 )\n",
    "\n",
    "#         if not video_meta_list:\n",
    "#             logging.error(\n",
    "#                 f\"All proxies failed for query or retuned empty list for query '{query}'. Skipping...\"\n",
    "#             )\n",
    "#             return\n",
    "#         # Continue with processing the successful result\n",
    "\n",
    "#         items = []\n",
    "#         logging.info(\"Fetching metadata...\")\n",
    "#         for video_meta in tqdm(\n",
    "#             video_meta_list,\n",
    "#             total=min(len(video_meta_list), limit),\n",
    "#             desc=f\"Processing {query}\",\n",
    "#         ):\n",
    "#             try:\n",
    "#                 # Extract metadata with fallback to None for missing keys\n",
    "#                 video_id = video_meta.get(\"videoId\", \"\").strip() or None\n",
    "\n",
    "#                 title = (\n",
    "#                     video_meta.get(\"title\", {})\n",
    "#                     .get(\"runs\", [{}])[0]\n",
    "#                     .get(\"text\", \"\")\n",
    "#                     .strip()\n",
    "#                     or None\n",
    "#                 )\n",
    "\n",
    "#                 channel = (\n",
    "#                     video_meta.get(\"longBylineText\", {})\n",
    "#                     .get(\"runs\", [{}])[0]\n",
    "#                     .get(\"text\", \"\")\n",
    "#                     .strip()\n",
    "#                     or None\n",
    "#                 )\n",
    "\n",
    "#                 at_channel = (\n",
    "#                     video_meta.get(\"longBylineText\", {})\n",
    "#                     .get(\"runs\", [{}])[0]\n",
    "#                     .get(\"navigationEndpoint\", {})\n",
    "#                     .get(\"browseEndpoint\", {})\n",
    "#                     .get(\"canonicalBaseUrl\", \"\")\n",
    "#                     .strip()\n",
    "#                     or None\n",
    "#                 )\n",
    "\n",
    "#                 view_count = video_meta.get(\"viewCountText\", {}).get(\"simpleText\", None)\n",
    "#                 # Add to item list\n",
    "#                 items.append(\n",
    "#                     {\n",
    "#                         \"query\": query,\n",
    "#                         \"video_id\": video_id,\n",
    "#                         \"title\": title,\n",
    "#                         \"channel\": channel,\n",
    "#                         \"at_channel\": at_channel,\n",
    "#                         \"view_count\": view_count,\n",
    "#                         \"video_url\": f\"{video_base_url}{video_id}\",\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#             except KeyError as e:\n",
    "#                 logging.warning(\n",
    "#                     f\"KeyError encountered while processing query {query}: {e}\"\n",
    "#                 )\n",
    "#                 continue\n",
    "\n",
    "#         df = pd.DataFrame(items)\n",
    "\n",
    "#         # Save data to CSV and JSON files\n",
    "#         save_to_csv(df, file_path_csv)\n",
    "#         save_to_json(video_meta_list, file_path_json)\n",
    "\n",
    "#         # return df, video_meta_list\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error fetching metadata for query '{query}': {e}\")\n",
    "#         # return pd.DataFrame(), []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Create locks for thread-safe file operations\n",
    "csv_lock = threading.Lock()\n",
    "json_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, file_path_csv: str):\n",
    "    \"\"\"\n",
    "    Save or append DataFrame to a CSV file in a thread-safe manner.\n",
    "    \"\"\"\n",
    "    with csv_lock:  # Ensure only one thread writes to the file at a time\n",
    "        try:\n",
    "            # Check if the file exists\n",
    "            with open(file_path_csv, \"r\"):\n",
    "                # Append without writing the header if the file exists\n",
    "                df.to_csv(file_path_csv, mode=\"a\", header=False, index=False)\n",
    "        except FileNotFoundError:\n",
    "            # Write with the header if the file does not exist\n",
    "            df.to_csv(file_path_csv, mode=\"w\", header=True, index=False)\n",
    "\n",
    "\n",
    "def save_to_json(video_meta_list: list[dict], file_path_json: str):\n",
    "    \"\"\"\n",
    "    Append data to a JSON file in a thread-safe manner.\n",
    "    \"\"\"\n",
    "    with json_lock:  # Ensure only one thread writes to the file at a time\n",
    "        try:\n",
    "            # Read existing data\n",
    "            with open(file_path_json, \"r\") as file:\n",
    "                existing_data = json.load(file)\n",
    "        except FileNotFoundError:\n",
    "            # Start with an empty list if the file does not exist\n",
    "            existing_data = []\n",
    "\n",
    "        # Add new data to the existing data\n",
    "        existing_data.extend(video_meta_list)\n",
    "\n",
    "        # Write the updated data back to the file\n",
    "        with open(file_path_json, \"w\") as file:\n",
    "            json.dump(existing_data, file, indent=4)\n",
    "\n",
    "def fetch_video_meta(\n",
    "    query: str,\n",
    "    limit: int,\n",
    "    sleep_min: int,\n",
    "    sleep_max: int,\n",
    "    sp_filter: str,\n",
    "    results_type: str,\n",
    "    proxies: list[dict[str, str]],\n",
    "    video_base_url: str,\n",
    "    file_path_csv: str,\n",
    "    file_path_json: str,\n",
    ") -> tuple[pd.DataFrame, list[dict]]:\n",
    "    \"\"\"\n",
    "    Fetch video metadata for a single query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Searching YouTube for: {query}\")\n",
    "        sleep_time = random.randint(sleep_min, sleep_max)\n",
    "\n",
    "        # Keep track of proxies tried\n",
    "        proxies_tried = set()\n",
    "        video_meta_list = []\n",
    "        while len(proxies_tried) < len(proxies):\n",
    "            # Select a random proxy that has not been tried\n",
    "            proxy = random.choice(\n",
    "                [p for p in proxies if tuple(p.items()) not in proxies_tried]\n",
    "            )\n",
    "            proxies_tried.add(tuple(proxy.items()))\n",
    "\n",
    "            try:\n",
    "                video_meta_list = list(\n",
    "                    get_search_cc(\n",
    "                        query=query,\n",
    "                        limit=limit,\n",
    "                        sleep=sleep_time,\n",
    "                        sp_filter=sp_filter,\n",
    "                        results_type=results_type,\n",
    "                        proxies=proxy,\n",
    "                    )\n",
    "                )\n",
    "                # If the list is not empty, break out of the loop\n",
    "                if video_meta_list:\n",
    "                    logging.info(f\"Found result for {query}\")\n",
    "                    break\n",
    "                else:\n",
    "                    logging.warning(\n",
    "                        f\"Proxy {proxy} returned an empty result for query '{query}'. There is no cc videos\"\n",
    "                    )\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                logging.warning(\n",
    "                    f\"Proxy {proxy} failed for query '{query}': {e}. Trying another proxy...\"\n",
    "                )\n",
    "\n",
    "        if not video_meta_list:\n",
    "            logging.error(\n",
    "                f\"All proxies failed for query or retuned empty list for query '{query}'. Skipping...\"\n",
    "            )\n",
    "            return\n",
    "        # Continue with processing the successful result\n",
    "\n",
    "        items = []\n",
    "        logging.info(\"Fetching metadata...\")\n",
    "        for video_meta in tqdm(\n",
    "            video_meta_list,\n",
    "            total=min(len(video_meta_list), limit),\n",
    "            desc=f\"Processing {query}\",\n",
    "        ):\n",
    "            try:\n",
    "                # Extract metadata with fallback to None for missing keys\n",
    "                video_id = video_meta.get(\"videoId\", \"\").strip() or None\n",
    "\n",
    "                title = (\n",
    "                    video_meta.get(\"title\", {})\n",
    "                    .get(\"runs\", [{}])[0]\n",
    "                    .get(\"text\", \"\")\n",
    "                    .strip()\n",
    "                    or None\n",
    "                )\n",
    "\n",
    "                channel = (\n",
    "                    video_meta.get(\"longBylineText\", {})\n",
    "                    .get(\"runs\", [{}])[0]\n",
    "                    .get(\"text\", \"\")\n",
    "                    .strip()\n",
    "                    or None\n",
    "                )\n",
    "\n",
    "                at_channel = (\n",
    "                    video_meta.get(\"longBylineText\", {})\n",
    "                    .get(\"runs\", [{}])[0]\n",
    "                    .get(\"navigationEndpoint\", {})\n",
    "                    .get(\"browseEndpoint\", {})\n",
    "                    .get(\"canonicalBaseUrl\", \"\")\n",
    "                    .strip()\n",
    "                    or None\n",
    "                )\n",
    "\n",
    "                view_count = video_meta.get(\"viewCountText\", {}).get(\"simpleText\", None)\n",
    "                # Add to item list\n",
    "                items.append(\n",
    "                    {\n",
    "                        \"query\": query,\n",
    "                        \"video_id\": video_id,\n",
    "                        \"title\": title,\n",
    "                        \"channel\": channel,\n",
    "                        \"at_channel\": at_channel,\n",
    "                        \"view_count\": view_count,\n",
    "                        \"video_url\": f\"{video_base_url}{video_id}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            except KeyError as e:\n",
    "                logging.warning(\n",
    "                    f\"KeyError encountered while processing query {query}: {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        df = pd.DataFrame(items)\n",
    "\n",
    "        # Save data to CSV and JSON files\n",
    "        save_to_csv(df, file_path_csv)\n",
    "        save_to_json(video_meta_list, file_path_json)\n",
    "\n",
    "        # return df, video_meta_list\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching metadata for query '{query}': {e}\")\n",
    "        # return pd.DataFrame(), []\n",
    "\n",
    "\n",
    "def get_cc_video_meta(\n",
    "    queries: list[str],\n",
    "    limit: int,\n",
    "    sleep_min: int,\n",
    "    sleep_max: int,\n",
    "    sp_filter: str,\n",
    "    file_path_csv: str,\n",
    "    file_path_json: str,\n",
    "    results_type: str = \"video\",\n",
    "    proxies: list[dict[str, str]] = None,\n",
    "    max_workers: int = 4,\n",
    ") -> tuple[pd.DataFrame, list[dict]]:\n",
    "    \n",
    "    video_base_url = \"https://www.youtube.com/watch?v=\"\n",
    "\n",
    "    # Parallel processing with ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                fetch_video_meta,\n",
    "                query,\n",
    "                limit,\n",
    "                sleep_min,\n",
    "                sleep_max,\n",
    "                sp_filter,\n",
    "                results_type,\n",
    "                proxies,\n",
    "                video_base_url,\n",
    "                file_path_csv,\n",
    "                file_path_json,\n",
    "            )\n",
    "            for query in queries\n",
    "        ]\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()  # Just ensure the task completes\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in processing query: {e}\")\n",
    "\n",
    "    logging.info(\"Finished fetching metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Use query string to get relevant channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_strings = [\n",
    "    \"คณิตศาสตร์\",\n",
    "    \"ฟิสิกส์\",\n",
    "    \"เคมี\",\n",
    "    \"ชีววิทยา\",\n",
    "    \"สังคมศาสตร์\",\n",
    "    \"ประวัติศาสตร์\",\n",
    "    \"การงานอาชีพ\",\n",
    "    \"พลศึกษา\",\n",
    "    \"ภาษาไทย\",\n",
    "    \"สุขศึกษา\",\n",
    "    \"การเมือง\",\n",
    "    \"การท่องเที่ยว\",\n",
    "    \"กฎหมาย\",\n",
    "    \"วิศวกรรมศาสตร์\",\n",
    "    \"คอมพิวเตอร์\",\n",
    "    \"การเขียนโปรแกรม\",\n",
    "    \"การใช้ชีวิต\",\n",
    "    \"จิตวิทยา\",\n",
    "    \"การเกษตร\",\n",
    "    \"การแพทย์\",\n",
    "    \"การรักษาโรค\",\n",
    "    \"สถาปัตยกรรม\",\n",
    "    \"การบริหารธุรกิจ\",\n",
    "    \"การประมง\",\n",
    "    \"การศึกษา\",\n",
    "    \"อุตสหกรรม\",\n",
    "    \"สิ่งแวดล้อม\",\n",
    "    \"การพยาบาล\",\n",
    "    \"กีฬา\",\n",
    "    \"การโรงแรม\",\n",
    "    \"สาธารณสุข\",\n",
    "    \"ทรัพยากรธรรมชาติ\",\n",
    "    \"เศรษฐศาสตร์\",\n",
    "    \"การเงิน\",\n",
    "    \"มหาวิทยาลัย\",\n",
    "    \"การพัฒนาตนเอง\",  # Self-improvement\n",
    "    \"ดาราศาสตร์\",      # Astronomy\n",
    "    \"การถ่ายภาพ\",      # Photography\n",
    "    \"การออกแบบกราฟิก\", # Graphic Design\n",
    "    \"ภาษาอังกฤษ\",      # English Language\n",
    "    \"วัฒนธรรม\",        # Culture\n",
    "    \"เทคโนโลยี\",       # Technology\n",
    "    \"การทำอาหาร\",      # Cooking\n",
    "    \"ดนตรี\",           # Music\n",
    "    \"การเงินส่วนบุคคล\"  # Personal Finance\n",
    "    # Trending or Popular Content\n",
    "    \"เกมออนไลน์\",         # Online Games\n",
    "    \"การลงทุน\",           # Investing\n",
    "    \"สุขภาพจิต\",         # Mental Health\n",
    "    \"ปัญญาประดิษฐ์\",      # Artificial Intelligence\n",
    "    \"การเรียนรู้ของเครื่อง\",  # Machine Learning\n",
    "\n",
    "    # Lifestyle and Personal Development\n",
    "    \"การออกกำลังกาย\",     # Fitness\n",
    "    \"โยคะ\",               # Yoga\n",
    "    \"การวางแผนชีวิต\",     # Life Planning\n",
    "    \"มังสวิรัติ\",         # Vegetarianism\n",
    "    \"การจัดการเวลา\",      # Time Management\n",
    "\n",
    "    # Creative and DIY\n",
    "    \"งานฝีมือ\",           # Handicrafts\n",
    "    \"การออกแบบภายใน\",     # Interior Design\n",
    "    \"การทำสวน\",           # Gardening\n",
    "    \"การเขียนนิยาย\",      # Novel Writing\n",
    "    \"การซ่อมแซมบ้าน\",     # Home Repairs\n",
    "\n",
    "    # Educational and Technical\n",
    "    \"หุ่นยนต์\",           # Robotics\n",
    "    \"การเขียนโค้ด\",       # Coding\n",
    "    \"ข้อมูลขนาดใหญ่\",      # Big Data\n",
    "    \"การสอนออนไลน์\",      # Online Teaching\n",
    "    \"การวิเคราะห์ข้อมูล\", # Data Analysis\n",
    "\n",
    "    # Cultural and Entertainment\n",
    "    \"ภาพยนตร์\",           # Movies\n",
    "    \"อนิเมะ\",             # Anime\n",
    "    \"การเดินทางท่องเที่ยว\", # Travel Adventures\n",
    "    \"เพลงไทย\",            # Thai Music\n",
    "    \"การเรียนภาษาใหม่\"    # Learning a New Language\n",
    "]\n",
    "limit = 1_000\n",
    "sleep_min = 5\n",
    "sleep_max = 15\n",
    "sp_filter = \"relevance\"\n",
    "workers = 128\n",
    "file_path_csv = f\"query_string_df_{today_string}.csv\"\n",
    "file_path_json = f\"query_string_df_{today_string}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cc_video_meta(\n",
    "    queries=query_strings,\n",
    "    limit=limit,\n",
    "    sleep_min=sleep_min,\n",
    "    sleep_max=sleep_max,\n",
    "    sp_filter=sp_filter,\n",
    "    file_path_csv=file_path_csv,\n",
    "    file_path_json=file_path_json,\n",
    "    max_workers=workers,\n",
    "    proxies=proxies,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CC videos in each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string_df = pd.read_csv(\"data/query_string_df_20241211.csv\")\n",
    "query_string_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string_df[\"channel\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## use this to continue download: we just filter out already downloaded\n",
    "## if load from scratch, skip this cell\n",
    "# filter = pd.read_csv(\"./data/channel_df_20241211.csv\")\n",
    "# print(filter.shape)\n",
    "# downloaded_channel = filter[\"channel\"].unique().tolist()\n",
    "# len(downloaded_channel)\n",
    "\n",
    "# channels = query_string_df[\"channel\"].unique().tolist()\n",
    "# print(len(channels))\n",
    "# channels = [c for c in channels if c not in downloaded_channel]\n",
    "# print(len(channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "channels = ('\"' + query_string_df[\"channel\"] + '\"').unique().tolist()\n",
    "channels, len(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Set up logging\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "# )\n",
    "\n",
    "\n",
    "# def get_cc_video_meta(\n",
    "#     queries: List[str],\n",
    "#     limit: int,\n",
    "#     sleep_min: int,\n",
    "#     sleep_max: int,\n",
    "#     sp_filter: str,\n",
    "#     results_type: str = \"video\",\n",
    "#     proxies: Optional[Dict[str, str]] = None,\n",
    "# ) -> Tuple[pd.DataFrame, List[dict]]:\n",
    "#     \"\"\"\n",
    "#     Retrieve video metadata for a list of queries using Scrapetube.\n",
    "\n",
    "#     Args:\n",
    "#         queries (List[str]): List of search queries to fetch metadata for.\n",
    "#         limit (int): Maximum number of results per query.\n",
    "#         sleep_min (int): Minimum sleep time between requests.\n",
    "#         sleep_max (int): Maximum sleep time between requests.\n",
    "#         sp_filter (str): Filter parameter for YouTube search.\n",
    "#         results_type (str, optional): Type of result to fetch (default is 'video').\n",
    "#         proxies (Optional[Dict[str, str]], optional): Proxy configuration (default is None).\n",
    "\n",
    "#     Returns:\n",
    "#         Tuple[List[pd.DataFrame], List[dict]]: A tuple containing:\n",
    "#             - A list of Pandas DataFrames with metadata.\n",
    "#             - A list of raw JSON responses.\n",
    "#     \"\"\"\n",
    "#     video_base_url = \"https://www.youtube.com/watch?v=\"\n",
    "#     all_dataframes = []\n",
    "#     all_jsons = []\n",
    "\n",
    "#     for query in tqdm(queries, total=len(queries)):\n",
    "#         logging.info(f\"Searching YouTube for: {query}\")\n",
    "\n",
    "#         # Random sleep time for throttling\n",
    "#         sleep_time = random.randint(sleep_min, sleep_max)\n",
    "#         try:\n",
    "#             video_meta_generator = get_search_cc(\n",
    "#                 query=query,\n",
    "#                 limit=limit,\n",
    "#                 sleep=sleep_time,\n",
    "#                 sp_filter=sp_filter,\n",
    "#                 results_type=results_type,\n",
    "#                 proxies=proxies,\n",
    "#             )\n",
    "\n",
    "#             # Convert generator to a list to check its content\n",
    "#             video_meta_list = list(video_meta_generator)\n",
    "#             if not video_meta_list:\n",
    "#                 logging.warning(f\"No results found for query '{query}'. Skipping...\")\n",
    "#                 continue\n",
    "\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Failed to fetch metadata for query '{query}': {e}\")\n",
    "#             continue\n",
    "\n",
    "#         items = []\n",
    "#         logging.info(\"Fetching metadata...\")\n",
    "#         for video_meta in tqdm(\n",
    "#             video_meta_list,\n",
    "#             total=min(len(video_meta_list), limit),\n",
    "#             desc=f\"Processing {query}\",\n",
    "#         ):\n",
    "#             try:\n",
    "#                 # Extract metadata\n",
    "#                 video_id = video_meta[\"videoId\"].strip()\n",
    "#                 title = video_meta[\"title\"][\"runs\"][0][\"text\"].strip()\n",
    "#                 channel = video_meta[\"longBylineText\"][\"runs\"][0][\"text\"].strip()\n",
    "#                 at_channel = video_meta[\"longBylineText\"][\"runs\"][0][\n",
    "#                     \"navigationEndpoint\"\n",
    "#                 ][\"browseEndpoint\"][\"canonicalBaseUrl\"].strip()\n",
    "#                 view_count = video_meta[\"viewCountText\"][\"simpleText\"]\n",
    "\n",
    "#                 # Add to item list\n",
    "#                 items.append(\n",
    "#                     {   \n",
    "#                         \"query_string\" : query,\n",
    "#                         \"video_id\": video_id,\n",
    "#                         \"title\": title,\n",
    "#                         \"channel\": channel,\n",
    "#                         \"at_channel\": at_channel,\n",
    "#                         \"view_count\": view_count,\n",
    "#                         \"video_url\": f\"{video_base_url}{video_id}\",\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#                 # Append raw JSON for detailed analysis later\n",
    "#                 all_jsons.append(video_meta)\n",
    "\n",
    "#             except KeyError as e:\n",
    "#                 logging.warning(f\"KeyError encountered while processing a video: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#         # Convert to DataFrame and append\n",
    "#         if items:\n",
    "#             all_dataframes.append(pd.DataFrame(items))\n",
    "\n",
    "#         # Sleep to avoid being blocked\n",
    "#         sleep_interval = random.randint(3, 7)\n",
    "#         logging.info(f\"Sleeping for {sleep_interval} seconds...\")\n",
    "#         sleep(sleep_interval)\n",
    "\n",
    "#     logging.info(\"Finished fetching metadata.\")\n",
    "#     return pd.DataFrame(all_dataframes), all_jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Set up logging\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "# )\n",
    "\n",
    "\n",
    "# def fetch_video_meta(\n",
    "#     query: str,\n",
    "#     limit: int,\n",
    "#     sleep_min: int,\n",
    "#     sleep_max: int,\n",
    "#     sp_filter: str,\n",
    "#     results_type: str,\n",
    "#     proxies: list[dict[str, str]],\n",
    "#     video_base_url: str,\n",
    "#     file_path_csv: str,\n",
    "#     file_path_json: str,\n",
    "# ) -> tuple[pd.DataFrame, list[dict]]:\n",
    "#     \"\"\"\n",
    "#     Fetch video metadata for a single query.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         logging.info(f\"Searching YouTube for: {query}\")\n",
    "\n",
    "#         proxy = random.choice(proxies)\n",
    "\n",
    "#         # Random sleep time for throttling\n",
    "#         sleep_time = random.randint(sleep_min, sleep_max)\n",
    "#         video_meta_generator = get_search_cc(\n",
    "#             query=query,\n",
    "#             limit=limit,\n",
    "#             sleep=sleep_time,\n",
    "#             sp_filter=sp_filter,\n",
    "#             results_type=results_type,\n",
    "#             proxies=proxy,\n",
    "#         )\n",
    "\n",
    "#         video_meta_list = list(video_meta_generator)\n",
    "#         if not video_meta_list:\n",
    "#             logging.warning(f\"No results found for query '{query}'. Skipping...\")\n",
    "#             return pd.DataFrame(), []\n",
    "\n",
    "#         items = []\n",
    "#         logging.info(\"Fetching metadata...\")\n",
    "#         for video_meta in tqdm(\n",
    "#             video_meta_list,\n",
    "#             total=min(len(video_meta_list), limit),\n",
    "#             desc=f\"Processing {query}\",\n",
    "#         ):\n",
    "#             try:\n",
    "#                 # Extract metadata with fallback to None for missing keys\n",
    "#                 video_id = video_meta.get(\"videoId\", \"\").strip() or None\n",
    "\n",
    "#                 title = (\n",
    "#                     video_meta.get(\"title\", {})\n",
    "#                     .get(\"runs\", [{}])[0]\n",
    "#                     .get(\"text\", \"\")\n",
    "#                     .strip()\n",
    "#                     or None\n",
    "#                 )\n",
    "\n",
    "#                 channel = (\n",
    "#                     video_meta.get(\"longBylineText\", {})\n",
    "#                     .get(\"runs\", [{}])[0]\n",
    "#                     .get(\"text\", \"\")\n",
    "#                     .strip()\n",
    "#                     or None\n",
    "#                 )\n",
    "\n",
    "#                 at_channel = (\n",
    "#                     video_meta.get(\"longBylineText\", {})\n",
    "#                     .get(\"runs\", [{}])[0]\n",
    "#                     .get(\"navigationEndpoint\", {})\n",
    "#                     .get(\"browseEndpoint\", {})\n",
    "#                     .get(\"canonicalBaseUrl\", \"\")\n",
    "#                     .strip()\n",
    "#                     or None\n",
    "#                 )\n",
    "\n",
    "#                 view_count = video_meta.get(\"viewCountText\", {}).get(\"simpleText\", None)\n",
    "#                 # Add to item list\n",
    "#                 items.append(\n",
    "#                     {\n",
    "#                         \"query\": query,\n",
    "#                         \"video_id\": video_id,\n",
    "#                         \"title\": title,\n",
    "#                         \"channel\": channel,\n",
    "#                         \"at_channel\": at_channel,\n",
    "#                         \"view_count\": view_count,\n",
    "#                         \"video_url\": f\"{video_base_url}{video_id}\",\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#             except KeyError as e:\n",
    "#                 logging.warning(\n",
    "#                     f\"KeyError encountered while processing query {query}: {e}\"\n",
    "#                 )\n",
    "#                 continue\n",
    "\n",
    "#         df = pd.DataFrame(items)\n",
    "\n",
    "#         # save append df\n",
    "#         try:\n",
    "#             # Check if the file exists\n",
    "#             with open(file_path_csv, \"r\"):\n",
    "#                 # If the file exists, append without writing the header\n",
    "#                 df.to_csv(file_path_csv, mode=\"a\", header=False, index=False)\n",
    "#         except FileNotFoundError:\n",
    "#             # If the file does not exist, write with the header\n",
    "#             df.to_csv(file_path_csv, mode=\"w\", header=True, index=False)\n",
    "\n",
    "#         # save append json\n",
    "#         # Append data to the JSON file\n",
    "#         try:\n",
    "#             # Read existing data\n",
    "#             with open(file_path_json, \"r\") as file:\n",
    "#                 existing_data = json.load(file)\n",
    "#         except FileNotFoundError:\n",
    "#             # File does not exist, start with an empty list\n",
    "#             existing_data = []\n",
    "\n",
    "#         # Add the new data to the existing data\n",
    "#         existing_data.extend(video_meta_list)\n",
    "\n",
    "#         # Write the updated data back to the file\n",
    "#         with open(file_path_json, \"w\") as file:\n",
    "#             json.dump(existing_data, file, indent=4)\n",
    "\n",
    "#         return df, video_meta_list\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error fetching metadata for query '{query}': {e}\")\n",
    "#         return pd.DataFrame(), []\n",
    "\n",
    "\n",
    "# def get_cc_video_meta(\n",
    "#     queries: list[str],\n",
    "#     limit: int,\n",
    "#     sleep_min: int,\n",
    "#     sleep_max: int,\n",
    "#     sp_filter: str,\n",
    "#     file_path_csv:str,\n",
    "#     file_path_json:str,\n",
    "#     results_type: str = \"video\",\n",
    "#     proxies: list[dict[str, str]] = None,\n",
    "#     max_workers: int = 4,\n",
    "# ) -> tuple[pd.DataFrame, list[dict]]:\n",
    "#     \"\"\"\n",
    "#     Retrieve video metadata for multiple queries using parallel processing.\n",
    "\n",
    "#     Args:\n",
    "#         queries (List[str]): List of search queries to fetch metadata for.\n",
    "#         limit (int): Maximum number of results per query.\n",
    "#         sleep_min (int): Minimum sleep time between requests.\n",
    "#         sleep_max (int): Maximum sleep time between requests.\n",
    "#         sp_filter (str): Filter parameter for YouTube search.\n",
    "#         results_type (str, optional): Type of result to fetch (default is 'video').\n",
    "#         proxies (Optional[Dict[str, str]], optional): Proxy configuration (default is None).\n",
    "#         max_workers (int): Number of parallel workers to use.\n",
    "\n",
    "#     Returns:\n",
    "#         Tuple[List[pd.DataFrame], List[dict]]: A tuple containing:\n",
    "#             - A list of Pandas DataFrames with metadata.\n",
    "#             - A list of raw JSON responses.\n",
    "#     \"\"\"\n",
    "#     video_base_url = \"https://www.youtube.com/watch?v=\"\n",
    "#     all_dataframes = []\n",
    "#     all_jsons = []\n",
    "\n",
    "#     # Parallel processing with ThreadPoolExecutor\n",
    "#     with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         futures = [\n",
    "#             executor.submit(\n",
    "#                 fetch_video_meta,\n",
    "#                 query,\n",
    "#                 limit,\n",
    "#                 sleep_min,\n",
    "#                 sleep_max,\n",
    "#                 sp_filter,\n",
    "#                 results_type,\n",
    "#                 proxies,\n",
    "#                 video_base_url,\n",
    "#             )\n",
    "#             for query in queries\n",
    "#         ]\n",
    "\n",
    "#         for future in as_completed(futures):\n",
    "#             try:\n",
    "#                 df, json_list = future.result()\n",
    "#                 if not df.empty:\n",
    "#                     all_dataframes.append(df)\n",
    "#                     all_jsons.extend(json_list)\n",
    "#             except Exception as e:\n",
    "#                 logging.error(f\"Error in processing: {e}\")\n",
    "\n",
    "#     logging.info(\"Finished fetching metadata.\")\n",
    "#     return pd.concat(all_dataframes), all_jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import threading\n",
    "\n",
    "# # Set up logging\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "# )\n",
    "\n",
    "# # Create locks for thread-safe file operations\n",
    "# csv_lock = threading.Lock()\n",
    "# json_lock = threading.Lock()\n",
    "\n",
    "\n",
    "# def save_to_csv(df: pd.DataFrame, file_path_csv: str):\n",
    "#     \"\"\"\n",
    "#     Save or append DataFrame to a CSV file in a thread-safe manner.\n",
    "#     \"\"\"\n",
    "#     with csv_lock:  # Ensure only one thread writes to the file at a time\n",
    "#         try:\n",
    "#             # Check if the file exists\n",
    "#             with open(file_path_csv, \"r\"):\n",
    "#                 # Append without writing the header if the file exists\n",
    "#                 df.to_csv(file_path_csv, mode=\"a\", header=False, index=False)\n",
    "#         except FileNotFoundError:\n",
    "#             # Write with the header if the file does not exist\n",
    "#             df.to_csv(file_path_csv, mode=\"w\", header=True, index=False)\n",
    "\n",
    "\n",
    "# def save_to_json(video_meta_list: list[dict], file_path_json: str):\n",
    "#     \"\"\"\n",
    "#     Append data to a JSON file in a thread-safe manner.\n",
    "#     \"\"\"\n",
    "#     with json_lock:  # Ensure only one thread writes to the file at a time\n",
    "#         try:\n",
    "#             # Read existing data\n",
    "#             with open(file_path_json, \"r\") as file:\n",
    "#                 existing_data = json.load(file)\n",
    "#         except FileNotFoundError:\n",
    "#             # Start with an empty list if the file does not exist\n",
    "#             existing_data = []\n",
    "\n",
    "#         # Add new data to the existing data\n",
    "#         existing_data.extend(video_meta_list)\n",
    "\n",
    "#         # Write the updated data back to the file\n",
    "#         with open(file_path_json, \"w\") as file:\n",
    "#             json.dump(existing_data, file, indent=4)\n",
    "\n",
    "\n",
    "# def fetch_video_meta(\n",
    "#     query: str,\n",
    "#     limit: int,\n",
    "#     sleep_min: int,\n",
    "#     sleep_max: int,\n",
    "#     sp_filter: str,\n",
    "#     results_type: str,\n",
    "#     proxies: list[dict[str, str]],\n",
    "#     video_base_url: str,\n",
    "#     file_path_csv: str,\n",
    "#     file_path_json: str,\n",
    "# ) -> tuple[pd.DataFrame, list[dict]]:\n",
    "#     \"\"\"\n",
    "#     Fetch video metadata for a single query.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         logging.info(f\"Searching YouTube for: {query}\")\n",
    "\n",
    "#         proxy = random.choice(proxies)\n",
    "\n",
    "#         # Random sleep time for throttling\n",
    "#         sleep_time = random.randint(sleep_min, sleep_max)\n",
    "#         video_meta_generator = get_search_cc(\n",
    "#             query=query,\n",
    "#             limit=limit,\n",
    "#             sleep=sleep_time,\n",
    "#             sp_filter=sp_filter,\n",
    "#             results_type=results_type,\n",
    "#             proxies=proxy,\n",
    "#         )\n",
    "\n",
    "#         video_meta_list = list(video_meta_generator)\n",
    "#         if not video_meta_list:\n",
    "#             logging.warning(f\"No results found for query '{query}'. Skipping...\")\n",
    "#             return pd.DataFrame(), []\n",
    "\n",
    "#         items = []\n",
    "#         logging.info(\"Fetching metadata...\")\n",
    "#         for video_meta in tqdm(\n",
    "#             video_meta_list,\n",
    "#             total=min(len(video_meta_list), limit),\n",
    "#             desc=f\"Processing {query}\",\n",
    "#         ):\n",
    "#             try:\n",
    "#                 # Extract metadata with fallback to None for missing keys\n",
    "#                 video_id = video_meta.get(\"videoId\", \"\").strip() or None\n",
    "\n",
    "#                 title = (\n",
    "#                     video_meta.get(\"title\", {})\n",
    "#                     .get(\"runs\", [{}])[0]\n",
    "#                     .get(\"text\", \"\")\n",
    "#                     .strip()\n",
    "#                     or None\n",
    "#                 )\n",
    "\n",
    "#                 channel = (\n",
    "#                     video_meta.get(\"longBylineText\", {})\n",
    "#                     .get(\"runs\", [{}])[0]\n",
    "#                     .get(\"text\", \"\")\n",
    "#                     .strip()\n",
    "#                     or None\n",
    "#                 )\n",
    "\n",
    "#                 at_channel = (\n",
    "#                     video_meta.get(\"longBylineText\", {})\n",
    "#                     .get(\"runs\", [{}])[0]\n",
    "#                     .get(\"navigationEndpoint\", {})\n",
    "#                     .get(\"browseEndpoint\", {})\n",
    "#                     .get(\"canonicalBaseUrl\", \"\")\n",
    "#                     .strip()\n",
    "#                     or None\n",
    "#                 )\n",
    "\n",
    "#                 view_count = video_meta.get(\"viewCountText\", {}).get(\"simpleText\", None)\n",
    "#                 # Add to item list\n",
    "#                 items.append(\n",
    "#                     {\n",
    "#                         \"query\": query,\n",
    "#                         \"video_id\": video_id,\n",
    "#                         \"title\": title,\n",
    "#                         \"channel\": channel,\n",
    "#                         \"at_channel\": at_channel,\n",
    "#                         \"view_count\": view_count,\n",
    "#                         \"video_url\": f\"{video_base_url}{video_id}\",\n",
    "#                     }\n",
    "#                 )\n",
    "\n",
    "#             except KeyError as e:\n",
    "#                 logging.warning(\n",
    "#                     f\"KeyError encountered while processing query {query}: {e}\"\n",
    "#                 )\n",
    "#                 continue\n",
    "\n",
    "#         df = pd.DataFrame(items)\n",
    "\n",
    "#         # Save data to CSV and JSON files\n",
    "#         save_to_csv(df, file_path_csv)\n",
    "#         save_to_json(video_meta_list, file_path_json)\n",
    "\n",
    "#         return df, video_meta_list\n",
    "\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error fetching metadata for query '{query}': {e}\")\n",
    "#         return pd.DataFrame(), []\n",
    "    \n",
    "\n",
    "\n",
    "# def get_cc_video_meta(\n",
    "#     queries: list[str],\n",
    "#     limit: int,\n",
    "#     sleep_min: int,\n",
    "#     sleep_max: int,\n",
    "#     sp_filter: str,\n",
    "#     file_path_csv:str,\n",
    "#     file_path_json:str,\n",
    "#     results_type: str = \"video\",\n",
    "#     proxies: list[dict[str, str]] = None,\n",
    "#     max_workers: int = 4,\n",
    "# ) -> tuple[pd.DataFrame, list[dict]]:\n",
    "#     \"\"\"\n",
    "#     Retrieve video metadata for multiple queries using parallel processing.\n",
    "\n",
    "#     Args:\n",
    "#         queries (List[str]): List of search queries to fetch metadata for.\n",
    "#         limit (int): Maximum number of results per query.\n",
    "#         sleep_min (int): Minimum sleep time between requests.\n",
    "#         sleep_max (int): Maximum sleep time between requests.\n",
    "#         sp_filter (str): Filter parameter for YouTube search.\n",
    "#         results_type (str, optional): Type of result to fetch (default is 'video').\n",
    "#         proxies (Optional[Dict[str, str]], optional): Proxy configuration (default is None).\n",
    "#         max_workers (int): Number of parallel workers to use.\n",
    "\n",
    "#     Returns:\n",
    "#         Tuple[List[pd.DataFrame], List[dict]]: A tuple containing:\n",
    "#             - A list of Pandas DataFrames with metadata.\n",
    "#             - A list of raw JSON responses.\n",
    "#     \"\"\"\n",
    "#     video_base_url = \"https://www.youtube.com/watch?v=\"\n",
    "#     all_dataframes = []\n",
    "#     all_jsons = []\n",
    "\n",
    "#     # Parallel processing with ThreadPoolExecutor\n",
    "#     with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         futures = [\n",
    "#             executor.submit(\n",
    "#                 fetch_video_meta,\n",
    "#                 query,\n",
    "#                 limit,\n",
    "#                 sleep_min,\n",
    "#                 sleep_max,\n",
    "#                 sp_filter,\n",
    "#                 results_type,\n",
    "#                 proxies,\n",
    "#                 video_base_url,\n",
    "#                 file_path_csv,\n",
    "#                 file_path_json,\n",
    "#             )\n",
    "#             for query in queries\n",
    "#         ]\n",
    "\n",
    "#         for future in as_completed(futures):\n",
    "#             try:\n",
    "#                 df, json_list = future.result()\n",
    "#                 if not df.empty:\n",
    "#                     all_dataframes.append(df)\n",
    "#                     all_jsons.extend(json_list)\n",
    "#             except Exception as e:\n",
    "#                 logging.error(f\"Error in processing: {e}\")\n",
    "\n",
    "#     logging.info(\"Finished fetching metadata.\")\n",
    "#     return pd.concat(all_dataframes), all_jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1_000\n",
    "sleep_min = 2\n",
    "sleep_max = 16\n",
    "sp_filter = \"relevance\"\n",
    "workers = 128\n",
    "file_path_csv = f\"./data/channel_df_{today_string}.csv\"\n",
    "file_path_json = f\"./data/channel_df_{today_string}.json\"\n",
    "print(file_path_csv, file_path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.choice(proxies)\n",
    "# ถ้าจะรันใหม่อย่าลืมไปลบ csv, json ด้วยยยย\n",
    "get_cc_video_meta(\n",
    "    queries=channels,\n",
    "    limit=limit,\n",
    "    sleep_min=sleep_min,\n",
    "    sleep_max=sleep_max,\n",
    "    sp_filter=sp_filter,\n",
    "    file_path_csv=file_path_csv,\n",
    "    file_path_json=file_path_json,\n",
    "    max_workers=workers,\n",
    "    proxies=proxies,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Use youtube_transcript_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi, NoTranscriptFound\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies = [\n",
    "    # {\"http\": \"http://103.25.210.233:9191\"},\n",
    "    # {\"http\": \"http://67.43.227.226:30373\"},\n",
    "    {\"http\": \"http://116.202.121.34:3128\"},\n",
    "    {\"http\": \"http://116.108.3.96:10017\"},\n",
    "    {\"http\": \"http://160.86.242.23:8080\"},\n",
    "    # {\"http\": \"http://35.209.198.222:80\"},\n",
    "    # {\"http\": \"http://20.27.86.185:8080\"},\n",
    "    {\"http\": \"http://31.47.58.37:80\"},\n",
    "    {\"http\": \"http://213.218.255.99:80\"},\n",
    "    {\"http\": \"http://72.10.160.94:8355\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./channel_cc_video.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = \"mbmckE6eJkQ\"\n",
    "lang_code = [\"en\"]\n",
    "available_subtitle = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "try:\n",
    "    subtitles = available_subtitle.find_transcript(lang_code)\n",
    "    print(subtitles.is_generated)\n",
    "    print(subtitles.video_id)\n",
    "except NoTranscriptFound as nt:\n",
    "    print(f\"NO subtitle for lang code {lang_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = YouTubeTranscriptApi.get_transcript(video_id, languages=[\"th\"])\n",
    "# type(out)\n",
    "# len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_ids = [\"ZP163YC1_9E\", \"lHsneMSnjgk\"]\n",
    "# try:\n",
    "#     out = YouTubeTranscriptApi.get_transcripts(video_ids, languages=lang_code)\n",
    "# except NoTranscriptFound:\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_subtitle_from_video_id(\n",
    "#         video_id:str,\n",
    "#         lang_code:list[str],\n",
    "#         proxies:list[dict[str, str]],\n",
    "# ):  \n",
    "#     proxy = random.choice(proxies)\n",
    "#     available_subtitle = YouTubeTranscriptApi.list_transcripts(video_id, proxies=proxy)\n",
    "#     try:\n",
    "#         subtitles = available_subtitle.find_transcript(lang_code)\n",
    "#         return {\n",
    "#             \"video_id\" : subtitles.video_id,\n",
    "#             \"is_generate\" : subtitles.is_generated,\n",
    "#             \"subtitle\" : subtitles.fetch()\n",
    "#         }\n",
    "#     except NoTranscriptFound:\n",
    "#         print(f\"NO subtitle for lang code {lang_code}\")\n",
    "#         return None\n",
    "    \n",
    "# lang_code = [\"th\"]\n",
    "# subtitle_list = []\n",
    "# for id in tqdm(df[\"video_id\"], total=len(df), desc=\"getting subtitle\"):\n",
    "#     subtitle_list.append(\n",
    "#         get_subtitle_from_video_id(\n",
    "#             video_id=id,\n",
    "#             lang_code=lang_code,\n",
    "#             proxies=proxies,\n",
    "#         )\n",
    "#     )\n",
    "#     sleep(random.randint(3, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "threading.active_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch subtitles\n",
    "def get_subtitle_from_video_id(\n",
    "    video_id: str, lang_code: list[str], proxies: list[dict[str, str]]\n",
    ") -> dict | None:\n",
    "    \"\"\"\n",
    "    Fetch subtitles for a given YouTube video ID.\n",
    "    \"\"\"\n",
    "    proxy = random.choice(proxies)  # Randomly select a proxy\n",
    "    try:\n",
    "        available_subtitle = YouTubeTranscriptApi.list_transcripts(\n",
    "            video_id, proxies=proxy\n",
    "        )\n",
    "        subtitles = available_subtitle.find_transcript(lang_code)\n",
    "        return {\n",
    "            \"video_id\": subtitles.video_id,\n",
    "            \"is_generate\": subtitles.is_generated,\n",
    "            \"subtitle\": subtitles.fetch(),\n",
    "        }\n",
    "    except NoTranscriptFound:\n",
    "        print(f\"No subtitle for lang code {lang_code} in video {video_id}\")\n",
    "        print()\n",
    "        return {\n",
    "            \"video_id\": video_id,\n",
    "            \"is_generate\": None,\n",
    "            \"subtitle\": None,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching subtitles for video {video_id}: {e}\")\n",
    "        print()\n",
    "        return {\n",
    "            \"video_id\": video_id,\n",
    "            \"is_generate\": None,\n",
    "            \"subtitle\": None,\n",
    "        }\n",
    "\n",
    "\n",
    "# Thread-safe function with throttling\n",
    "def fetch_with_throttle(\n",
    "    video_id: str,\n",
    "    lang_code: list[str],\n",
    "    proxies: list[dict[str, str]],\n",
    "    sleep_min: int = 3,\n",
    "    sleep_max: int = 10,\n",
    ") -> dict | None:\n",
    "    \"\"\"\n",
    "    Wrapper function to introduce throttling between requests.\n",
    "    \"\"\"\n",
    "    result = get_subtitle_from_video_id(video_id, lang_code, proxies)\n",
    "    sleep(random.randint(sleep_min, sleep_max))  # Random delay to avoid rate limits\n",
    "    return result\n",
    "\n",
    "\n",
    "max_workers = 8\n",
    "lang_code = [\"th\"]\n",
    "subtitle_list = []\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [\n",
    "        executor.submit(fetch_with_throttle, video_id, lang_code, proxies)\n",
    "        for video_id in df[\"video_id\"]\n",
    "    ]\n",
    "    for future in tqdm(\n",
    "        as_completed(futures), total=len(futures), desc=\"Fetching subtitles\"\n",
    "    ):\n",
    "        try:\n",
    "            subtitle_list.append(future.result())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing future: {e}\")\n",
    "\n",
    "# Save results to DataFrame\n",
    "subtitle_df = pd.DataFrame(subtitle_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = joblib.dump(pd.DataFrame([i if i else {\"video_id\" : None, \"is_generate\" : None, \"subtitle\" : None} for i in subtitle_list]), \"tmp.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame([i if i else {\"video_id\" : None, \"is_generate\" : None, \"subtitle\" : None} for i in subtitle_list])\n",
    "# tmp_df = tmp_df.loc[tmp_df[\"is_generate\"].eq(False)]\n",
    "subs = tmp_df[\"subtitle\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "end = 0\n",
    "for sub in subs[:1]:\n",
    "    text_line = \"\"\n",
    "    for s in sub:\n",
    "        text = s[\"text\"]\n",
    "        start = s[\"start\"]\n",
    "        duration = s[\"duration\"]\n",
    "        if min(start - end, 0) < 1:\n",
    "            text_line += text\n",
    "        else:\n",
    "            text_line += \"\\n\"\n",
    "            text_line += text\n",
    "\n",
    "        end = start + duration\n",
    "    text_list.append(text_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scapetubeenv",
   "language": "python",
   "name": "scapetubeenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
